{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>No</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hours Studied  Previous Scores Extracurricular Activities  Sleep Hours  \\\n",
       "0              7               99                        Yes            9   \n",
       "1              4               82                         No            4   \n",
       "2              8               51                        Yes            7   \n",
       "3              5               52                        Yes            5   \n",
       "4              7               75                         No            8   \n",
       "\n",
       "   Sample Question Papers Practiced  \n",
       "0                                 1  \n",
       "1                                 2  \n",
       "2                                 2  \n",
       "3                                 2  \n",
       "4                                 5  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_x_df = pd.read_csv('dataset_X.csv')\n",
    "data_t_df = pd.read_csv('dataset_T.csv')\n",
    "data_x_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5\n",
    "total_size = data_x_df.shape[0]\n",
    "for i in range(total_size):\n",
    "    data_x_df.iloc[i, 2] = 1 if data_x_df.iloc[i, 2] == 'Yes' else 0\n",
    "    \n",
    "data_x = np.array(data_x_df, dtype='float64')\n",
    "data_t = np.array(data_t_df, dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature selection\n",
    "(a) Evaluate the RMS error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression:\n",
    "    def __init__(self, x_train, t_train):\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "\n",
    "    def basis_vec(self, x_vec, M):\n",
    "        phi = np.append([1], x_vec)\n",
    "        if M >= 2:\n",
    "            for i in range(D):\n",
    "                phi = np.append(phi, x_vec[i] * x_vec[i: D])\n",
    "        if M == 3:\n",
    "            for i in range(D):\n",
    "                for j in range(i, D):\n",
    "                    phi = np.append(phi, x_vec[i] * x_vec[j] * x_vec[j: D])\n",
    "            \n",
    "        return phi\n",
    "\n",
    "    def get_w(self, M, lambda_ = 0):\n",
    "        self.M = M\n",
    "        phi_mat = np.array([self.basis_vec(xn, M) for xn in self.x_train])\n",
    "        n = np.size(phi_mat, 1)\n",
    "        self.w = np.dot(np.dot(np.linalg.pinv(np.dot(phi_mat.T, phi_mat) + np.eye(n)*lambda_), phi_mat.T), self.t_train)\n",
    "        return self\n",
    "        \n",
    "    def RMS_error(self, x, t):\n",
    "        sum = 0\n",
    "        n = np.size(x, 0)\n",
    "        for i in range(n):\n",
    "            sum += (np.dot(self.w.T, self.basis_vec(x[i], self.M)) - t[i])[0] ** 2\n",
    "        return np.sqrt(sum / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error on the training set: 2.0317614508071022\n",
      "RMS error on the valid set: 2.060869473062396\n"
     ]
    }
   ],
   "source": [
    "N = int(total_size * 0.8)\n",
    "x_train = data_x[: N]\n",
    "t_train = data_t[: N]\n",
    "x_valid = data_x[N:]\n",
    "t_valid = data_t[N:]\n",
    "\n",
    "M = 1\n",
    "reg = Regression(x_train, t_train)\n",
    "w_M1 = reg.get_w(M).w\n",
    "print('RMS error on the training set:', reg.RMS_error(x_train, t_train))\n",
    "print('RMS error on the valid set:', reg.RMS_error(x_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error on the training set: 2.0295179009903648\n",
      "RMS error on the valid set: 2.0648657388094898\n"
     ]
    }
   ],
   "source": [
    "M = 2\n",
    "w_M2 = reg.get_w(M).w\n",
    "print('RMS error on the training set:', reg.RMS_error(x_train, t_train))\n",
    "print('RMS error on the valid set:', reg.RMS_error(x_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain:  \n",
    "1. The values of the RMS error on the training set for M = 1 and M = 2 are all within 2 ~ 2.2, which means that the regression satisfied our target of expection, so we can conclude that the train data is enough for this model, that is, there is no under-fitting problem for this model.\n",
    "\n",
    "2. Compared the RMS error on the training set and the valid set, the value of RMS set is close to each other no matter the model is applied with M = 1 or M = 2, and even though the values on the valid set are a little bigger than that on the training set, the smaller increase is within our acceptable range. Therefore, we can conclude that the model may be suitable for the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Select the most contributive feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:\n",
      " [[-34.01047532   2.85148294   1.01802319   0.63130645   0.47417895\n",
      "    0.19139737]] \n",
      "\n",
      "Feature of Hours Studied deleted:\n",
      "RMS error on the training set: 7.649717618347436\n",
      "RMS error on the valid set: 7.706686993409626 \n",
      "\n",
      "Feature of Previous Scores deleted:\n",
      "RMS error on the training set: 17.78392667241751\n",
      "RMS error on the valid set: 17.749047293349722 \n",
      "\n",
      "Feature of Extracurricular Activities deleted:\n",
      "RMS error on the training set: 2.0561157122066143\n",
      "RMS error on the valid set: 2.077918435001256 \n",
      "\n",
      "Feature of Sleep Hours deleted:\n",
      "RMS error on the training set: 2.1849757102033\n",
      "RMS error on the valid set: 2.232053425832941 \n",
      "\n",
      "Feature of Sample Question Papers Practiced deleted:\n",
      "RMS error on the training set: 2.1040542669701194\n",
      "RMS error on the valid set: 2.143491867735507 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "M = 1\n",
    "print('Weight:\\n', w_M1.T, '\\n')\n",
    "name = ['Hours Studied', 'Previous Scores', 'Extracurricular Activities', 'Sleep Hours', 'Sample Question Papers Practiced']\n",
    "for i in range(D):\n",
    "    print('Feature of %s deleted:'%name[i])\n",
    "    x_tmp_train = np.delete(x_train, i, axis=1)\n",
    "    x_tmp_valid = np.delete(x_valid, i, axis=1)\n",
    "    reg = Regression(x_tmp_train, t_train)\n",
    "    reg.get_w(M)\n",
    "    print('RMS error on the training set:', reg.RMS_error(x_tmp_train, t_train))\n",
    "    print('RMS error on the valid set:', reg.RMS_error(x_tmp_valid, t_valid), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain:  \n",
    "1. From the weight calculated above, we can see the max-value among w-vector is the bias term, and the second highest is the first feature, the hours studied. However, since the data haven't been normalized, the values of weight can't necessarily present the contribution for the following score.\n",
    "\n",
    "2. Hence, I subsequently delete the five features and then calculate the RMS errors of the remain four features resectively.  From the result above, the RMS increase a lot when the feature -- Previous Score is deleted, which means that the prevoius score is the most contributive feature for the polynomial model with M = 1.\n",
    "\n",
    "3. All the coefficients on w-vector are postive except for the bias term, so we can conclude that there is no negative contribution among the feature for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Maximum likelihood approach\n",
    "(a) I choose Polynomial functions as basis functions for regression because all the five features -- hours studied, previous scores, extracurricular activities, sleep hours, and sample question papers practiced are usually positively related to student performance, and polynomial models have this property. Plus, in the previous question, I obtain a small RMS for M = 2 using a polynomial model, which makes me prefer to increase order and try to construct a improved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error on the training set: 2.0382734812749996\n",
      "RMS error on the valid set: 2.0851359963301173\n"
     ]
    }
   ],
   "source": [
    "M = 3\n",
    "reg = Regression(x_train, t_train)\n",
    "reg.get_w(M)\n",
    "print('RMS error on the training set:', reg.RMS_error(x_train, t_train))\n",
    "print('RMS error on the valid set:', reg.RMS_error(x_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain:  \n",
    "1. In the model with M = 3, I add 35 more basis functions than with M = 2 where M denotes the highest order in this model. When M = 2, besides the bias term and all the features, there are the squares of the features and cross-terms derived by multiplying one another. Therefore, when M = 3, I add the cubes of the five features and the remaining cross-terms, the number of which is H(5, 3) = 35, so the total number of the parameters is 1+5+15+35=56.\n",
    "\n",
    "2. From the results above, the values of RMS errors are small on the training set and both on the valid set, and the difference between the training set and the valid set is small, which means that the regression satisfied our target of expection and there is no under-fitting problem for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) N-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 5\n",
    "def N_fold(M, lambda_ = 0):\n",
    "    n = total_size\n",
    "    mean_err = 0\n",
    "    for i in range(FOLD):\n",
    "        x_tmp_train = np.delete(data_x, slice(n*i//FOLD, n*(i+1)//FOLD), axis=0)\n",
    "        t_tmp_train = np.delete(data_t, slice(n*i//FOLD, n*(i+1)//FOLD), axis=0)\n",
    "        x_tmp_valid = data_x[n*i//FOLD: n*(i+1)//FOLD]\n",
    "        t_tmp_valid = data_t[n*i//FOLD: n*(i+1)//FOLD]\n",
    "        reg = Regression(x_tmp_train, t_tmp_train)\n",
    "        reg.get_w(M, lambda_)\n",
    "        mean_err += reg.RMS_error(x_tmp_valid, t_tmp_valid) / FOLD\n",
    "    return mean_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error on the training set: 2.0317614508071022\n",
      "RMS error on the valid set: 2.060869473062396\n",
      "Mean RMS error for 5-fold on the valid set: 2.0382242992941846\n"
     ]
    }
   ],
   "source": [
    "M = 1\n",
    "reg.get_w(M)\n",
    "print('RMS error on the training set:', reg.RMS_error(x_train, t_train))\n",
    "print('RMS error on the valid set:', reg.RMS_error(x_valid, t_valid))\n",
    "print('Mean RMS error for %i-fold on the valid set:' %FOLD, N_fold(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error on the training set: 2.0295179009903648\n",
      "RMS error on the valid set: 2.0648657388094898\n",
      "Mean RMS error for 5-fold on the valid set: 2.039830244665117\n"
     ]
    }
   ],
   "source": [
    "M = 2\n",
    "reg.get_w(M)\n",
    "print('RMS error on the training set:', reg.RMS_error(x_train, t_train))\n",
    "print('RMS error on the valid set:', reg.RMS_error(x_valid, t_valid))\n",
    "print('Mean RMS error for %i-fold on the valid set:' %FOLD, N_fold(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error on the training set: 2.0382734812749996\n",
      "RMS error on the valid set: 2.0851359963301173\n",
      "Mean RMS error for 5-fold on the valid set: 2.058998135305097\n"
     ]
    }
   ],
   "source": [
    "M = 3\n",
    "reg.get_w(M)\n",
    "print('RMS error on the training set:', reg.RMS_error(x_train, t_train))\n",
    "print('RMS error on the valid set:', reg.RMS_error(x_valid, t_valid))\n",
    "print('Mean RMS error for %i-fold on the valid set:' %FOLD, N_fold(M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain:  \n",
    "1. After five-fold cross-validation, the mean RMS error among the five subsets are all smaller than the situation when we just take one set for validation no matter when M = 1, 2, 3. Hence, the last 20% of the data are less suitable as a validation set than the other four subsets. In addtion, N-fold cross-validation helps us judge a suitable model more comprehensively.\n",
    "\n",
    "2. From the result above, we can find the RMS errors getting smaller on the training set but a little bit higher on the valid set compared with M = 1 and M = 2, which means that the order M = 3 is about to be enough for the data in this model, and if we apply further higher order for the model, the over-fitting problem may be likely to emerge explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MAP approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) The difference between maximum likelihood approach and maximum a posteriori approach is whether the probabilty of prior is considered or not. If a prior obeys Gaussian distribution, we can prove that to maximize the posterior is same as to minimize E(w) + λ||w||^2, and the effect of the prior is equivalent to the regularization term -- lambda in error funcions, the function of which is to reduce over-fitting with the order of the model or the number of parameters increasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) MAP approach with 5-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS without λ:  2.060869473062396\n",
      "RMS for ln λ = 1e-06:  2.060869473733219\n"
     ]
    }
   ],
   "source": [
    "M = 1\n",
    "L = 0.000001\n",
    "reg = Regression(x_train, t_train)\n",
    "print('RMS without λ: ', reg.get_w(M).RMS_error(x_valid, t_valid))\n",
    "print('RMS for ln λ = %s: ' %L, reg.get_w(M, lambda_=L).RMS_error(x_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS without λ:  2.0648657388094898\n",
      "RMS for ln λ = 1e-06:  2.064865744915659\n"
     ]
    }
   ],
   "source": [
    "M = 2\n",
    "L = 0.000001\n",
    "print('RMS without λ: ', reg.get_w(M).RMS_error(x_valid, t_valid))\n",
    "print('RMS for ln λ = %s: ' %L, reg.get_w(M, lambda_=L).RMS_error(x_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For M = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS without λ:    2.0851359963301173\n",
      "RMS for λ = 1.6:  2.0809972594030444\n"
     ]
    }
   ],
   "source": [
    "M = 3\n",
    "L = 1.6\n",
    "print('RMS without λ:   ', reg.get_w(M).RMS_error(x_valid, t_valid))\n",
    "print('RMS for λ = %s: ' %L, reg.get_w(M, lambda_=L).RMS_error(x_valid, t_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Compared the results between maximum likelihood approach and maximum a posteriori approach, for M = 3, after adding λ term, I find there is such a lamda that we can obtain the smallest E(w) + λ||w||^2, but for M = 1 and 2, we can not find that λ since no matter how small the λ I assigned, E(w) + λ||w||^2 still increases. Thus, we can conclude that there is no over-fitting problem for M = 1 and 2, and for M = 3, since the RMS errors (with or without λ) are bigger that for M = 1 and 2, it seems to have a trend of over-fitting as the order gets higher for M > 3. Thus, except for the low orders M = 1 or 2, the result will be consistent with my conclusion in (a) with M >= 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
